# Отчёт по заданию 1 "Сравнение CNN и полносвязных сетей"

## 1.1 Сравнение на MNIST

По графика видно, что во время обучения CNN с Residual Block показывает себя лучше, чем Полносвязная.

Полносвязная:
- время обучения = 113.3 
- test_accuracy = 0.9768
- num_params = 235146

Простая CNN:
- train_time = 266.2 
- test_accuracy = 0.9898
- num_params = 421642
- 
CNNWithResidual:
- train_time = 1010.3 
- test_accuracy = 0.9919
- num_params = 160906

## 1.2 Сравнение на CIFAR-10

Проанализировав графики, я сделала вывод, что модель CNN с регуляризацией и Residual блоками допускает меньше ошибок, чем Полносвязная сеть и CNN с Residual блоками.

Полносвязная сеть (глубокая):
- время обучения = 415.7
- test_accuracy = 0.5351

CNN с Residual блоками:
- train_time = 787.9
- test_accuracy = 0.7648

CNN с регуляризацией и Residual блоками:
- train_time = 2584.7
- test_accuracy = 0.8074

# Отчёт по заданию 2 "Анализ архитектур CNN"

## 2.1 Влияние размера ядра свертки

 - 3×3 ядра: Чаще всего дают хорошую точность, потому что позволяют наращивать глубину сети без резкого роста числа параметров.
 - 1×1 + 3×3: Часто используется для уменьшения числа каналов (1×1), затем детальное выделение признаков (3×3). Это позволяет сохранить точность при меньших вычислениях.


## 2.2 Влияние глубины CNN

Модели с маленькой глубиной лучше на простых задачах и маленьких датасетах. Глубокие могут извлекать более сложные иерархические признаки, дают более высокую точность на сложных датасетах.

Depth_2:
* время обучения = 736.20 сек
* точность на тесте = 0.4506

Depth_4:
* время обучения = 1261.52 сек
* точность на тесте = 0.5636

Depth_6: 
* время обучения = 1787.42 сек
* точность на тесте = 0.6123

CNNWithResidual:
* время обучения = 2232.95 сек
* точность на тесте = 0.8174

# Отчёт по заданию 3 "Кастомные слои и эксперименты"

## 3.1 Реализация кастомных слоев

CustomConv2d:
- Похоже на обычный nn.Conv2d, но с доп. нормализацией, что может помочь ускорить схождение.

SpatialAttention:
- Похож на Squeeze-and-Excitation или CBAM, но здесь упрощённая spatial-only карта важности.
- Attention-механизм добавляет веса по пространству, но не меняет размеры.

CustomActivation:
- По смыслу напоминает RELU, но с другой формой экспоненты.
- Значения как положительные, так и отрицательные, значит функция не полностью обнуляет отрицательные значения (в отличие от ReLU). 
- Нет полной блокировки градиента

CustomMaxAvgPool2d:
- Объединяет идеи Max и Avg pool, может дать более сбалансированные признаки.
- Уменьшает количество каналов (объединяет их через Max и Avg)



## 3.2 Эксперименты с Residual блоками

Базовый блок:
- Меньше параметров
- Прост в оптимизации
- Хорош для небольших сетей или shallow-ResNet
- params: 73984

Bottleneck:
- Меньше параметров при схожей глубине
- Глубокие ResNet (ResNet-50/101) используют именно его
- Лучше градиентный поток (благодаря узкому промежуточному слою)
- params: 4544. Bottleneck блок оптимизирован по количеству параметров

Wide:
- Больше параметров
- Обычно быстрее сходится и показывает лучшее качество на малых эпохах
- Но требует больше памяти
- params: 295552. Wide-блок увеличивает ширину каналов внутри блока, повышая способность модели, но требуя больше памяти и вычислений.

На практике Bottleneck и Wide блоки более устойчивы к исчезающим градиентам, особенно на больших глубинах.