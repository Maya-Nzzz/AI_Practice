# Отчёт заданию 1 "Эксперименты с глубиной сети"

## Графики

Графики сохранены в папке:

```
plots/depth_experiments/
```

- Для модели с 1 слоем: `1_layer_curve.png`
- Для модели с 7 слоями: `7_layer_curve.png`

## Время обучения

- Обучение с **1 слоем** заняло **1.372 с**.
- Обучение с **7 слоями** заняло **1.625 с**.
- Обучение с **5 слоями** заняло **0,806 с**.
- Обучение с **5 слоями + batchnorm** заняло **1.191 с**.
- Обучение с **5 слоями + dropout+batchnorm** заняло **1.161 с**.

## Выводы

Таким образом, архитектура и регуляризация могут влиять на скорость обучения не только за счёт числа слоёв, но и за счёт улучшения сходимости или дополнительных вычислительных шагов. Использование этих слоев по отдельности и в комбинациях позволяет значительно поднять качество моделей и контролировать переобучение (когда модель запомнила обучающие данные и не реагирует на тестовые).

 # Отчёт заданию 2 "Эксперименты с шириной сети"
 
Более широкие сети показывают более высокую точность на тесте.
Время обучения увеличивается с ростом ширины, но рост не критический.
Количество параметров растёт экспоненциально, что может привести к переобучению на меньших датасетах.
Была построена тепловая карта точности в зависимости от архитектуры.
По оси X — ширина первого слоя, по оси Y — ширина последнего.
Цвет — точность.
Сохранённый график: plots/width_experiments/heatmap_accuracy.png

# Отчёт к заданию 3 "Эксперименты с глубиной сети"

## Точность

- Без регуляризации: **Train accuracy**: 0.9844; **Test accuracy**: 0.9844
- Только Dropout

| Коэффициент Dropout | Train accuracy | Test accuracy |
|---------------------|----------------|---------------|
| 0.1                 | 0.9669         | 0.9669        |
| 0.3                 | 0.9494         | 0.9494        |
| 0.5                 | 0.9087         | 0.9087        |

- Только BatchNorm: **Train accuracy**: 0.9912; **Test accuracy**: 0.9912
- Dropout + BatchNorm: **Train accuracy**: 0.8800; **Test accuracy**: 0.8800
- L2 регуляризация: **Train accuracy**: 0.9775; **Test accuracy**: 0.9775

## Эксперимент с комбинированными техниками

### Динамика обучения
 - Train losses: [0.6628, 0.5229, 0.4529, 0.3873, 0.3378, 0.3212, 0.2799, 0.2681, 0.2453, 0.2481]
 - Train accuracy: [0.6250, 0.7675, 0.7981, 0.8281, 0.8638, 0.8669, 0.8863, 0.8975, 0.9031, 0.8925]
 - Test losses: [0.5581, 0.4023, 0.3271, 0.2782, 0.2345, 0.2106, 0.1894, 0.1709, 0.1541, 0.1435]
 - Test accuracy: [0.7975, 0.8650, 0.8825, 0.8925, 0.9000, 0.9075, 0.9275, 0.9375, 0.9325, 0.9350]
   
## Выводы

- Использование BatchNorm показало наивысшую точность и стабильность обучения.
- Dropout снижает переобучение, однако при высоких коэффициентах (0.5) сильно падает точность.
- Комбинирование Dropout и BatchNorm привело к сильному снижению точности на тесте, что может быть связано с избыточной регуляризацией.
- L2 регуляризация также улучшает обобщающую способность модели и сохраняет высокую точность.
- В комбинированном эксперименте наблюдается стабильное снижение loss и рост accuracy, что демонстрирует постепенное улучшение качества обучения.
